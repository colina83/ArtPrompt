{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face NPL Course Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.- Tokenizers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 48.0/48.0 [00:00<00:00, 16.1kB/s]\n",
      "c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Francisco.Colina\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 1.20MB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(2, 17), dtype=int32, numpy=\n",
      "array([[  101,  1056,  5620, 19237,  1037,  2843,  1012,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [  101,  2425,  2033,  2008,  2017,  2066,  2033,  1010,  2302,\n",
      "         4129,  2033,  2008,  2017,  2066,  2033,  1012,   102]])>, 'attention_mask': <tf.Tensor: shape=(2, 17), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])>}\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = [\n",
    "    \"TGS sucks a lot.\",\n",
    "    \"Tell me that you like me, without telling me that you like me.\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "### Model \n",
    "from transformers import TFAutoModel\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = TFAutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vector output by the Transformer module is usually large. It generally has three dimensions:\n",
    "\n",
    "Batch size: The number of sequences processed at a time (2 in our example).\n",
    "Sequence length: The length of the numerical representation of the sequence (16 in our example).\n",
    "Hidden size: The vector dimension of each model input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 17, 768)\n"
     ]
    }
   ],
   "source": [
    "outputs = model(inputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n"
     ]
    }
   ],
   "source": [
    "print(outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFSequenceClassifierOutput(loss=None, logits=<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[ 2.8571773, -2.4997537],\n",
       "       [-3.199863 ,  3.3846471]], dtype=float32)>, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.-Postprocessing the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.9953068  0.00469323]\n",
      " [0.0013797  0.9986203 ]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "predictions = tf.math.softmax(outputs.logits, axis=-1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, TFBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the config\n",
    "config = BertConfig()\n",
    "\n",
    "# Building the model from the config\n",
    "model = TFBertModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.32.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 570/570 [00:00<00:00, 191kB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 436M/436M [00:15<00:00, 27.6MB/s] \n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFBertModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [\"Hello!\", \"Cool.\", \"Nice!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_sequences = [\n",
    "    [101, 7592, 999, 102],\n",
    "    [101, 4658, 1012, 102],\n",
    "    [101, 3835, 999, 102],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model_inputs = tf.constant(encoded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(model_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translating text to numbers is known as encoding, and it is done in a 2-step process: the tokenization, followed by the conversion to input IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', '’', 've', 'been', 'waiting', 'for', 'a', 'Hu', '##gging', '##F', '##ace', 'course', 'my', 'whole', 'life', '.']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "sequence = \"I’ve been waiting for a HuggingFace course my whole life.\"\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[146, 787, 1396, 1151, 2613, 1111, 170, 20164, 10932, 2271, 7954, 1736, 1139, 2006, 1297, 119]\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I ’ ve been waiting for a HuggingFace course my whole life.\n"
     ]
    }
   ],
   "source": [
    "decoded_string = tokenizer.decode(ids)\n",
    "print(decoded_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Multiple Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TFSequenceClassifierOutput(loss=None, logits=<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-2.7276192,  2.8789363]], dtype=float32)>, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_ids = tf.constant(ids)\n",
    "# This line will fail.\n",
    "model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[  101  1045  1005  2310  2042  3403  2005  1037 17662 12172  2607  2026\n",
      "   2878  2166  1012   102]], shape=(1, 16), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "tokenized_inputs = tokenizer(sequence, return_tensors=\"tf\")\n",
    "print(tokenized_inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tf.Tensor(\n",
      "[[ 1045  1005  2310  2042  3403  2005  1037 17662 12172  2607  2026  2878\n",
      "   2166  1012]], shape=(1, 14), dtype=int32)\n",
      "Logits: tf.Tensor([[-2.7276192  2.8789363]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "input_ids = tf.constant([ids])\n",
    "print(\"Input IDs:\", input_ids)\n",
    "\n",
    "output = model(input_ids)\n",
    "print(\"Logits:\", output.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "model_inputs = tokenizer(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(3, 4), dtype=int32, numpy=\n",
       "array([[ 101, 7592,  999,  102],\n",
       "       [ 101, 4658, 1012,  102],\n",
       "       [ 101, 3835,  999,  102]])>, 'attention_mask': <tf.Tensor: shape=(3, 4), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 1]])>}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "output = model(**tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFSequenceClassifierOutput(loss=None, logits=<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[-1.5606955,  1.6122804],\n",
       "       [-3.6183178,  3.9137495]], dtype=float32)>, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "result = tokenizer.tokenize(\"Hello!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', '!']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 9.36kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 570/570 [00:00<00:00, 284kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 1.59MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 4.00MB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 440M/440M [00:35<00:00, 12.2MB/s] \n",
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14.910134315490723"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "# Same as before\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"This course is amazing!\",\n",
    "]\n",
    "batch = dict(tokenizer(sequences, padding=True, truncation=True, return_tensors=\"tf\"))\n",
    "\n",
    "# This is new\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\")\n",
    "labels = tf.convert_to_tensor([1, 1])\n",
    "model.train_on_batch(batch, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 28.8k/28.8k [00:00<00:00, 3.60MB/s]\n",
      "Downloading metadata: 100%|██████████| 28.7k/28.7k [00:00<00:00, 7.17MB/s]\n",
      "Downloading readme: 100%|██████████| 27.9k/27.9k [00:00<00:00, 4.00MB/s]\n",
      "Downloading data: 6.22kB [00:00, 583kB/s]0/3 [00:00<?, ?it/s]\n",
      "Downloading data: 1.05MB [00:00, 4.76MB/s]/3 [00:00<00:01,  1.82it/s]\n",
      "Downloading data: 441kB [00:00, 3.52MB/s]2/3 [00:01<00:00,  1.51it/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.58it/s]\n",
      "Generating train split: 100%|██████████| 3668/3668 [00:00<00:00, 17318.86 examples/s]\n",
      "Generating validation split: 100%|██████████| 408/408 [00:00<00:00, 6594.94 examples/s]\n",
      "Generating test split: 100%|██████████| 1725/1725 [00:00<00:00, 30870.87 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
       " 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n",
       " 'label': 1,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset = raw_datasets[\"train\"]\n",
    "raw_train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenized_sentences_1 = tokenizer(raw_datasets[\"train\"][\"sentence1\"])\n",
    "tokenized_sentences_2 = tokenizer(raw_datasets[\"train\"][\"sentence2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"This is the first sentence.\", \"This is the second one.\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'this',\n",
       " 'is',\n",
       " 'the',\n",
       " 'first',\n",
       " 'sentence',\n",
       " '.',\n",
       " '[SEP]',\n",
       " 'this',\n",
       " 'is',\n",
       " 'the',\n",
       " 'second',\n",
       " 'one',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenizer(\n",
    "    raw_datasets[\"train\"][\"sentence1\"],\n",
    "    raw_datasets[\"train\"][\"sentence2\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3668/3668 [00:00<00:00, 5679.33 examples/s]\n",
      "Map: 100%|██████████| 408/408 [00:00<00:00, 3590.96 examples/s]\n",
      "Map: 100%|██████████| 1725/1725 [00:00<00:00, 5692.84 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50, 59, 47, 67, 59, 50, 62, 32]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = tokenized_datasets[\"train\"][:8]\n",
    "samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n",
    "[len(x) for x in samples[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': TensorShape([8, 67]),\n",
       " 'token_type_ids': TensorShape([8, 67]),\n",
       " 'attention_mask': TensorShape([8, 67]),\n",
       " 'labels': TensorShape([8])}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator(samples)\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\datasets\\arrow_dataset.py:400: FutureWarning: The output of `to_tf_dataset` will change when a passing single element list for `labels` or `columns` in the next datasets version. To return a tuple structure rather than dict, pass a single string.\n",
      "Old behaviour: columns=['a'], labels=['labels'] -> (tf.Tensor, tf.Tensor)  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor)  \n",
      "New behaviour: columns=['a'],labels=['labels'] -> ({'a': tf.Tensor}, {'labels': tf.Tensor})  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor) \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tf_train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
    "    label_cols=[\"labels\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "tf_validation_dataset = tokenized_datasets[\"validation\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
    "    label_cols=[\"labels\"],\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"bert-base-cased\"\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) # Value before the sotfmax layer in the model\n",
    "model.compile(optimizer=\"adam\", loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5/459 [..............................] - ETA: 24:43 - loss: 0.7697"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'tf_bert_for_sequence_classification_1/bert/embeddings/assert_less/Assert/Assert' defined at (most recent call last):\n    File \"C:\\Users\\Francisco.Colina\\AppData\\Local\\Programs\\Python\\Python39\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\Francisco.Colina\\AppData\\Local\\Programs\\Python\\Python39\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\traitlets\\config\\application.py\", line 1043, in launch_instance\n      app.start()\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 728, in start\n      self.io_loop.start()\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\Francisco.Colina\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n      self._run_once()\n    File \"C:\\Users\\Francisco.Colina\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n      handle._run()\n    File \"C:\\Users\\Francisco.Colina\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 516, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 505, in process_one\n      await dispatch(*args)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 412, in dispatch_shell\n      await result\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 740, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Francisco.Colina\\AppData\\Local\\Temp\\ipykernel_21432\\3352150589.py\", line 1, in <module>\n      model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1742, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1338, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1322, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1303, in run_step\n      outputs = model.train_step(data)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1637, in train_step\n      y_pred = self(x, training=True)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 569, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1150, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1557, in run_call_with_unpacked_inputs\n      \"method added in TF 2.8. If you want the original HF compute_loss, please call \"\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 1569, in call\n      outputs = self.bert(\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1150, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1557, in run_call_with_unpacked_inputs\n      \"method added in TF 2.8. If you want the original HF compute_loss, please call \"\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 780, in call\n      embedding_output = self.embeddings(\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1150, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 201, in call\n      if input_ids is not None:\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 202, in call\n      check_embeddings_within_bounds(input_ids, self.config.vocab_size)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\transformers\\tf_utils.py\", line 161, in check_embeddings_within_bounds\n      tf.debugging.assert_less(\nNode: 'tf_bert_for_sequence_classification_1/bert/embeddings/assert_less/Assert/Assert'\nassertion failed: [The maximum value of input_ids (Tensor(\\\"tf_bert_for_sequence_classification_1/bert/embeddings/Max:0\\\", shape=(), dtype=int32)) must be smaller than the embedding layer\\'s input dimension (28996). The likely cause is some problem at tokenization time.] [Condition x < y did not hold element-wise:] [x (tf_bert_for_sequence_classification_1/Cast:0) = ] [[101 1996 2132...]...] [y (tf_bert_for_sequence_classification_1/bert/embeddings/Cast/x:0) = ] [28996]\n\t [[{{node tf_bert_for_sequence_classification_1/bert/embeddings/assert_less/Assert/Assert}}]] [Op:__inference_train_function_96840]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m      2\u001b[0m     tf_train_dataset,\n\u001b[0;32m      3\u001b[0m     validation_data\u001b[39m=\u001b[39;49mtf_validation_dataset,\n\u001b[0;32m      4\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'tf_bert_for_sequence_classification_1/bert/embeddings/assert_less/Assert/Assert' defined at (most recent call last):\n    File \"C:\\Users\\Francisco.Colina\\AppData\\Local\\Programs\\Python\\Python39\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\Francisco.Colina\\AppData\\Local\\Programs\\Python\\Python39\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\traitlets\\config\\application.py\", line 1043, in launch_instance\n      app.start()\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 728, in start\n      self.io_loop.start()\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\Francisco.Colina\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n      self._run_once()\n    File \"C:\\Users\\Francisco.Colina\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n      handle._run()\n    File \"C:\\Users\\Francisco.Colina\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 516, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 505, in process_one\n      await dispatch(*args)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 412, in dispatch_shell\n      await result\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 740, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Francisco.Colina\\AppData\\Local\\Temp\\ipykernel_21432\\3352150589.py\", line 1, in <module>\n      model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1742, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1338, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1322, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1303, in run_step\n      outputs = model.train_step(data)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1637, in train_step\n      y_pred = self(x, training=True)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 569, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1150, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1557, in run_call_with_unpacked_inputs\n      \"method added in TF 2.8. If you want the original HF compute_loss, please call \"\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 1569, in call\n      outputs = self.bert(\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1150, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1557, in run_call_with_unpacked_inputs\n      \"method added in TF 2.8. If you want the original HF compute_loss, please call \"\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 780, in call\n      embedding_output = self.embeddings(\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1150, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 201, in call\n      if input_ids is not None:\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 202, in call\n      check_embeddings_within_bounds(input_ids, self.config.vocab_size)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\transformers\\tf_utils.py\", line 161, in check_embeddings_within_bounds\n      tf.debugging.assert_less(\nNode: 'tf_bert_for_sequence_classification_1/bert/embeddings/assert_less/Assert/Assert'\nassertion failed: [The maximum value of input_ids (Tensor(\\\"tf_bert_for_sequence_classification_1/bert/embeddings/Max:0\\\", shape=(), dtype=int32)) must be smaller than the embedding layer\\'s input dimension (28996). The likely cause is some problem at tokenization time.] [Condition x < y did not hold element-wise:] [x (tf_bert_for_sequence_classification_1/Cast:0) = ] [[101 1996 2132...]...] [y (tf_bert_for_sequence_classification_1/bert/embeddings/Cast/x:0) = ] [28996]\n\t [[{{node tf_bert_for_sequence_classification_1/bert/embeddings/assert_less/Assert/Assert}}]] [Op:__inference_train_function_96840]"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    tf_train_dataset,\n",
    "    validation_data=tf_validation_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
    "\n",
    "batch_size = 8\n",
    "num_epochs = 3\n",
    "# The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied\n",
    "# by the total number of epochs. Note that the tf_train_dataset here is a batched tf.data.Dataset,\n",
    "# not the original Hugging Face Dataset, so its len() is already num_samples // batch_size.\n",
    "num_train_steps = len(tf_train_dataset) * num_epochs\n",
    "lr_scheduler = PolynomialDecay(\n",
    "    initial_learning_rate=5e-5, end_learning_rate=0.0, decay_steps=num_train_steps\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "opt = Adam(learning_rate=lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=opt, loss=loss, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'tf_bert_for_sequence_classification_2/bert/embeddings/assert_less/Assert/Assert' defined at (most recent call last):\n    File \"C:\\Users\\Francisco.Colina\\AppData\\Local\\Programs\\Python\\Python39\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\Francisco.Colina\\AppData\\Local\\Programs\\Python\\Python39\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\traitlets\\config\\application.py\", line 1043, in launch_instance\n      app.start()\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 728, in start\n      self.io_loop.start()\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\Francisco.Colina\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n      self._run_once()\n    File \"C:\\Users\\Francisco.Colina\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n      handle._run()\n    File \"C:\\Users\\Francisco.Colina\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 516, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 505, in process_one\n      await dispatch(*args)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 412, in dispatch_shell\n      await result\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 740, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Francisco.Colina\\AppData\\Local\\Temp\\ipykernel_21432\\1411177282.py\", line 1, in <module>\n      model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1742, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1338, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1322, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1303, in run_step\n      outputs = model.train_step(data)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1637, in train_step\n      y_pred = self(x, training=True)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 569, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1150, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1557, in run_call_with_unpacked_inputs\n      \"method added in TF 2.8. If you want the original HF compute_loss, please call \"\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 1569, in call\n      outputs = self.bert(\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1150, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1557, in run_call_with_unpacked_inputs\n      \"method added in TF 2.8. If you want the original HF compute_loss, please call \"\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 780, in call\n      embedding_output = self.embeddings(\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1150, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 201, in call\n      if input_ids is not None:\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 202, in call\n      check_embeddings_within_bounds(input_ids, self.config.vocab_size)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\transformers\\tf_utils.py\", line 161, in check_embeddings_within_bounds\n      tf.debugging.assert_less(\nNode: 'tf_bert_for_sequence_classification_2/bert/embeddings/assert_less/Assert/Assert'\nassertion failed: [The maximum value of input_ids (Tensor(\\\"tf_bert_for_sequence_classification_2/bert/embeddings/Max:0\\\", shape=(), dtype=int32)) must be smaller than the embedding layer\\'s input dimension (28996). The likely cause is some problem at tokenization time.] [Condition x < y did not hold element-wise:] [x (tf_bert_for_sequence_classification_2/Cast:0) = ] [[101 8769 1999...]...] [y (tf_bert_for_sequence_classification_2/bert/embeddings/Cast/x:0) = ] [28996]\n\t [[{{node tf_bert_for_sequence_classification_2/bert/embeddings/assert_less/Assert/Assert}}]] [Op:__inference_train_function_134109]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49mfit(tf_train_dataset, validation_data\u001b[39m=\u001b[39;49mtf_validation_dataset, epochs\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'tf_bert_for_sequence_classification_2/bert/embeddings/assert_less/Assert/Assert' defined at (most recent call last):\n    File \"C:\\Users\\Francisco.Colina\\AppData\\Local\\Programs\\Python\\Python39\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\Francisco.Colina\\AppData\\Local\\Programs\\Python\\Python39\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\traitlets\\config\\application.py\", line 1043, in launch_instance\n      app.start()\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 728, in start\n      self.io_loop.start()\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\Francisco.Colina\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n      self._run_once()\n    File \"C:\\Users\\Francisco.Colina\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n      handle._run()\n    File \"C:\\Users\\Francisco.Colina\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 516, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 505, in process_one\n      await dispatch(*args)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 412, in dispatch_shell\n      await result\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 740, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Francisco.Colina\\AppData\\Local\\Temp\\ipykernel_21432\\1411177282.py\", line 1, in <module>\n      model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1742, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1338, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1322, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1303, in run_step\n      outputs = model.train_step(data)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1637, in train_step\n      y_pred = self(x, training=True)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 569, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1150, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1557, in run_call_with_unpacked_inputs\n      \"method added in TF 2.8. If you want the original HF compute_loss, please call \"\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 1569, in call\n      outputs = self.bert(\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1150, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1557, in run_call_with_unpacked_inputs\n      \"method added in TF 2.8. If you want the original HF compute_loss, please call \"\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 780, in call\n      embedding_output = self.embeddings(\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1150, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 201, in call\n      if input_ids is not None:\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 202, in call\n      check_embeddings_within_bounds(input_ids, self.config.vocab_size)\n    File \"c:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\transformers\\tf_utils.py\", line 161, in check_embeddings_within_bounds\n      tf.debugging.assert_less(\nNode: 'tf_bert_for_sequence_classification_2/bert/embeddings/assert_less/Assert/Assert'\nassertion failed: [The maximum value of input_ids (Tensor(\\\"tf_bert_for_sequence_classification_2/bert/embeddings/Max:0\\\", shape=(), dtype=int32)) must be smaller than the embedding layer\\'s input dimension (28996). The likely cause is some problem at tokenization time.] [Condition x < y did not hold element-wise:] [x (tf_bert_for_sequence_classification_2/Cast:0) = ] [[101 8769 1999...]...] [y (tf_bert_for_sequence_classification_2/bert/embeddings/Cast/x:0) = ] [28996]\n\t [[{{node tf_bert_for_sequence_classification_2/bert/embeddings/assert_less/Assert/Assert}}]] [Op:__inference_train_function_134109]"
     ]
    }
   ],
   "source": [
    "model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nCamembertTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[79], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m CamembertTokenizer, TFCamembertForMaskedLM\n\u001b[1;32m----> 3\u001b[0m tokenizer \u001b[39m=\u001b[39m CamembertTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mcamembert-base\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m model \u001b[39m=\u001b[39m TFCamembertForMaskedLM\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mcamembert-base\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\transformers\\utils\\import_utils.py:1070\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[1;34m(cls, key)\u001b[0m\n\u001b[0;32m   1068\u001b[0m \u001b[39mif\u001b[39;00m key\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m key \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_from_config\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m   1069\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__getattribute__\u001b[39m(key)\n\u001b[1;32m-> 1070\u001b[0m requires_backends(\u001b[39mcls\u001b[39;49m, \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_backends)\n",
      "File \u001b[1;32mc:\\Users\\Francisco.Colina\\Documents\\Code\\ArtPrompt\\.venv\\lib\\site-packages\\transformers\\utils\\import_utils.py:1058\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   1056\u001b[0m failed \u001b[39m=\u001b[39m [msg\u001b[39m.\u001b[39mformat(name) \u001b[39mfor\u001b[39;00m available, msg \u001b[39min\u001b[39;00m checks \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m available()]\n\u001b[0;32m   1057\u001b[0m \u001b[39mif\u001b[39;00m failed:\n\u001b[1;32m-> 1058\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(failed))\n",
      "\u001b[1;31mImportError\u001b[0m: \nCamembertTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import CamembertTokenizer, TFCamembertForMaskedLM\n",
    "\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "model = TFCamembertForMaskedLM.from_pretrained(\"camembert-base\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
